{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from huggingface_hub import snapshot_download\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "4726978f375f6eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Wczytanie danych",
   "id": "37a6b29b57020bb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Download dataset repo\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"ejhusom/llm-inference-energy-consumption\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "# 2. Point to folder with CSVs\n",
    "data_dir = os.path.join(local_dir, \"data\")\n",
    "\n",
    "# 3. Read and label each CSV\n",
    "dfs = []\n",
    "for path in glob.glob(os.path.join(data_dir, \"*.csv\")):\n",
    "    fname = os.path.basename(path)\n",
    "    print(\"Reading:\", fname)\n",
    "\n",
    "    # infer label: laptop or workstation\n",
    "    if \"laptop1\" in fname.lower():\n",
    "        device_type = \"laptop1\"\n",
    "    if \"laptop2\" in fname.lower():\n",
    "        device_type = \"laptop2\"\n",
    "    elif \"workstation\" in fname.lower():\n",
    "        device_type = \"workstation\"\n",
    "    elif \"server\" in fname.lower():\n",
    "        device_type = \"server\"\n",
    "    else:\n",
    "        device_type = \"unknown\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"device_type\"] = device_type   # add as a new column\n",
    "    dfs.append(df)\n",
    "\n",
    "# 4. Merge all CSVs\n",
    "full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 5. Save merged version\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "full_df.to_csv(os.path.join(\"data\", \"llm_inference_energy.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Combined shape:\", full_df.shape)\n",
    "print(\"✅ Unique device types:\", full_df[\"device_type\"].unique())"
   ],
   "id": "6684d2be7732918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(os.path.join(\"data/llm_inference_energy.csv\"))\n",
    "\n",
    "data.info()"
   ],
   "id": "bb17cb61d406aa7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "More spec columns",
   "id": "2ec603cd5750a96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device_specs = {\n",
    "    \"server\": {\n",
    "        \"cpu_vendor\": \"AMD\",\n",
    "        \"cpu_family\": \"EPYC 7643\",\n",
    "        \"cpu_cores\": 48,\n",
    "        \"cpu_base_clock_ghz\": np.nan,  # unknown from your string; fill later if needed\n",
    "        \"ram_gb\": 528,\n",
    "        \"has_gpu\": 1,\n",
    "        \"gpu_model\": \"RTX A5000\",\n",
    "        \"gpu_vram_gb\": 24,\n",
    "        \"gpu_class\": \"pro\",\n",
    "        \"device_family\": \"server\",\n",
    "        \"is_mobile\": 0,\n",
    "    },\n",
    "    \"workstation\": {\n",
    "        \"cpu_vendor\": \"Intel\",\n",
    "        \"cpu_family\": \"Xeon W-2223\",\n",
    "        \"cpu_cores\": 8,\n",
    "        \"cpu_base_clock_ghz\": 3.6,\n",
    "        \"ram_gb\": 128,\n",
    "        \"has_gpu\": 1,\n",
    "        \"gpu_model\": \"RTX A2000\",\n",
    "        \"gpu_vram_gb\": 12,\n",
    "        \"gpu_class\": \"pro\",\n",
    "        \"device_family\": \"workstation\",\n",
    "        \"is_mobile\": 0,\n",
    "    },\n",
    "    \"laptop1\": {\n",
    "        \"cpu_vendor\": \"Intel\",\n",
    "        \"cpu_family\": \"Core i5 11th Gen\",\n",
    "        \"cpu_cores\": 12,\n",
    "        \"cpu_base_clock_ghz\": 2.4,\n",
    "        \"ram_gb\": 16,\n",
    "        \"has_gpu\": 0,\n",
    "        \"gpu_model\": \"None\",\n",
    "        \"gpu_vram_gb\": 0,\n",
    "        \"gpu_class\": \"none\",\n",
    "        \"device_family\": \"laptop\",\n",
    "        \"is_mobile\": 1,\n",
    "    },\n",
    "    \"laptop2\": {\n",
    "        \"cpu_vendor\": \"Intel\",\n",
    "        \"cpu_family\": \"Core i7 10th Gen\",\n",
    "        \"cpu_cores\": 12,\n",
    "        \"cpu_base_clock_ghz\": 2.7,\n",
    "        \"ram_gb\": 32,\n",
    "        \"has_gpu\": 1,\n",
    "        \"gpu_model\": \"Quadro RTX 4000\",\n",
    "        \"gpu_vram_gb\": 8,\n",
    "        \"gpu_class\": \"pro\",\n",
    "        \"device_family\": \"laptop\",\n",
    "        \"is_mobile\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "data[\"device_spec\"] = data[\"device_type\"].map(device_specs)\n",
    "data = data.join(pd.json_normalize(data[\"device_spec\"]))\n",
    "data.drop(columns=[\"device_spec\"], inplace=True)\n",
    "\n",
    "data.head()"
   ],
   "id": "c791a06d601cb87a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Proper model names",
   "id": "90fb770d1b9d827d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rename_map = {\n",
    "    \"codellama\": \"codellama:7b\",\n",
    "    \"llama3\": \"llama3:8b\",\n",
    "}\n",
    "\n",
    "data['model_name'] = data['model_name'].replace(rename_map)"
   ],
   "id": "71c0bbca548d914b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get model param number",
   "id": "2d237d2a2cad1275"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_map = {\n",
    "    \"gemma:2b\":       2e9,\n",
    "    \"gemma:7b\":       7e9,\n",
    "    \"codellama:7b\":   7e9,\n",
    "    \"llama3:8b\":      8e9,\n",
    "    \"llama3:70b\":     70e9,\n",
    "    \"codellama:70b\":  70e9,\n",
    "}\n",
    "\n",
    "data[\"model_params\"] = data[\"model_name\"].map(param_map).astype(float)  # number of parameters\n",
    "data[\"model_params_billion\"] = data[\"model_params\"] / 1e9"
   ],
   "id": "1747cc3aec8e239a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from feature_extraction import extract_features\n",
    "\n",
    "\n",
    "# Definicje kolumn na początku pliku\n",
    "response_columns = [\n",
    "    'prompt_token_length',\n",
    "    'response_token_length',\n",
    "    'energy_consumption_llm',\n",
    "    'readability_min_grade',\n",
    "    'readability_max_grade',\n",
    "    'readability_diff',\n",
    "    'response'\n",
    "]\n",
    "\n",
    "hardware_model_columns = [\n",
    "    'device_type',\n",
    "    'model_name',\n",
    "    'text_standard',  # Potrzebne do obliczenia readability\n",
    "]\n",
    "\n",
    "\n",
    "def prepare_data(dataframe: pd.DataFrame):\n",
    "    basic_columns = [\n",
    "        \"Unnamed: 0.2\", \"Unnamed: 0.1\", \"Unnamed: 0\", \"index\",\n",
    "        \"created_at\", \"start_time\", \"end_time\", \"energy_consumption_llm_total\",\n",
    "        \"type\", \"energy_consumption_monitoring\",\n",
    "        \"text_standard\", \"energy_consumption_llm_gpu\", \"energy_consumption_llm_cpu\"\n",
    "    ]\n",
    "    basic_columns = list(set(basic_columns) & set(dataframe.columns))\n",
    "    dataframe.drop(columns=basic_columns, inplace=True)\n",
    "    dataframe = dataframe.loc[~dataframe[\"model_name\"].isin([\"llama3:70b\", \"codellama:70b\"])].reset_index(drop=True)\n",
    "\n",
    "    target_corr = dataframe.corr(numeric_only=True)[\"energy_consumption_llm\"].to_frame()\n",
    "\n",
    "    low_correlated = target_corr.loc[\n",
    "        (target_corr[\"energy_consumption_llm\"] >= -0.01) &\n",
    "        (target_corr[\"energy_consumption_llm\"] <= 0.01)\n",
    "    ].index\n",
    "\n",
    "    dataframe = dataframe.drop(columns=low_correlated)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def extract_model_features(dataframe: pd.DataFrame):\n",
    "    dataframe = dataframe.copy(deep=True)\n",
    "    device_specs = {\n",
    "        \"server\": {\n",
    "            \"cpu_vendor\": \"AMD\",\n",
    "            \"cpu_family\": \"EPYC 7643\",\n",
    "            \"cpu_cores\": 48,\n",
    "            \"cpu_base_clock_ghz\": np.nan,\n",
    "            \"ram_gb\": 528,\n",
    "            \"has_gpu\": 1,\n",
    "            \"gpu_model\": \"RTX A5000\",\n",
    "            \"gpu_vram_gb\": 24,\n",
    "            \"gpu_class\": \"pro\",\n",
    "            \"device_family\": \"server\",\n",
    "            \"is_mobile\": 0,\n",
    "        },\n",
    "        \"workstation\": {\n",
    "            \"cpu_vendor\": \"Intel\",\n",
    "            \"cpu_family\": \"Xeon W-2223\",\n",
    "            \"cpu_cores\": 8,\n",
    "            \"cpu_base_clock_ghz\": 3.6,\n",
    "            \"ram_gb\": 128,\n",
    "            \"has_gpu\": 1,\n",
    "            \"gpu_model\": \"RTX A2000\",\n",
    "            \"gpu_vram_gb\": 12,\n",
    "            \"gpu_class\": \"pro\",\n",
    "            \"device_family\": \"workstation\",\n",
    "            \"is_mobile\": 0,\n",
    "        },\n",
    "        \"laptop1\": {\n",
    "            \"cpu_vendor\": \"Intel\",\n",
    "            \"cpu_family\": \"Core i5 11th Gen\",\n",
    "            \"cpu_cores\": 12,\n",
    "            \"cpu_base_clock_ghz\": 2.4,\n",
    "            \"ram_gb\": 16,\n",
    "            \"has_gpu\": 0,\n",
    "            \"gpu_model\": \"None\",\n",
    "            \"gpu_vram_gb\": 0,\n",
    "            \"gpu_class\": \"none\",\n",
    "            \"device_family\": \"laptop\",\n",
    "            \"is_mobile\": 1,\n",
    "        },\n",
    "        \"laptop2\": {\n",
    "            \"cpu_vendor\": \"Intel\",\n",
    "            \"cpu_family\": \"Core i7 10th Gen\",\n",
    "            \"cpu_cores\": 12,\n",
    "            \"cpu_base_clock_ghz\": 2.7,\n",
    "            \"ram_gb\": 32,\n",
    "            \"has_gpu\": 1,\n",
    "            \"gpu_model\": \"Quadro RTX 4000\",\n",
    "            \"gpu_vram_gb\": 8,\n",
    "            \"gpu_class\": \"pro\",\n",
    "            \"device_family\": \"laptop\",\n",
    "            \"is_mobile\": 1,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    dataframe[\"device_spec\"] = dataframe[\"device_type\"].map(device_specs)\n",
    "    dataframe = dataframe.join(pd.json_normalize(dataframe[\"device_spec\"]))\n",
    "    dataframe.drop(columns=[\"device_spec\"], inplace=True)\n",
    "\n",
    "    rename_map = {\n",
    "        \"codellama\": \"codellama:7b\",\n",
    "        \"llama3\": \"llama3:8b\",\n",
    "    }\n",
    "\n",
    "    dataframe['model_name'] = dataframe['model_name'].replace(rename_map)\n",
    "\n",
    "    param_map = {\n",
    "        \"gemma:2b\":       2e9,\n",
    "        \"gemma:7b\":       7e9,\n",
    "        \"codellama:7b\":   7e9,\n",
    "        \"llama3:8b\":      8e9,\n",
    "        \"llama3:70b\":     70e9,\n",
    "        \"codellama:70b\":  70e9,\n",
    "    }\n",
    "\n",
    "    dataframe[\"model_params\"] = dataframe[\"model_name\"].map(param_map).astype(float)\n",
    "    dataframe[\"model_params_billion\"] = dataframe[\"model_params\"] / 1e9\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def load_same_model_data(model_name: str, path_to_test: str = os.path.join(\"data\", \"test.csv\"), n_prompt_rows: int = 1000):\n",
    "    dataframe_models = pd.read_csv(path_to_test, usecols=['model_name'])\n",
    "    matching_indices = dataframe_models[dataframe_models[\"model_name\"] == model_name].index.tolist()\n",
    "\n",
    "    print(f\"Znaleziono {len(matching_indices)} wierszy dla model_name='{model_name}'\")\n",
    "\n",
    "    if len(matching_indices) == 0:\n",
    "        raise ValueError(f\"Nie znaleziono żadnych wierszy dla model_name='{model_name}'\")\n",
    "\n",
    "    if len(matching_indices) > n_prompt_rows:\n",
    "        matching_indices = random.sample(matching_indices, n_prompt_rows)\n",
    "\n",
    "    skip_rows = [i for i in range(1, len(dataframe_models) + 1) if (i - 1) not in matching_indices]\n",
    "\n",
    "    return skip_rows\n",
    "\n",
    "\n",
    "def extract_response_features(filtered_test: pd.DataFrame):\n",
    "    # Sprawdź które kolumny istnieją\n",
    "    existing_response_cols = [col for col in response_columns if col in filtered_test.columns]\n",
    "    return filtered_test[existing_response_cols]\n",
    "\n",
    "\n",
    "def ready_data(model_name: str, prompt: str):\n",
    "    # Cechy prompta\n",
    "    prompts = pd.DataFrame([prompt], columns=[\"prompt\"])\n",
    "    prompt_features = extract_features(prompts)\n",
    "\n",
    "    print(f\"Kształt cech prompta: {prompt_features.shape}\")\n",
    "\n",
    "    # Wczytanie losowej części zbioru testowego do wyciągnięcia response i model config\n",
    "    path_to_test = os.path.join(\"data\", \"test.csv\")\n",
    "    n_prompt_rows = prompt_features.shape[0]\n",
    "\n",
    "    skip_rows = load_same_model_data(model_name, path_to_test, n_prompt_rows)\n",
    "    filtered_test = pd.read_csv(path_to_test, skiprows=skip_rows)\n",
    "\n",
    "    print(f\"Wczytano {len(filtered_test)} wierszy z testu\")\n",
    "    print(f\"Dostępne kolumny: {filtered_test.columns.tolist()}\")\n",
    "\n",
    "    # Sprawdź które kolumny hardware/model rzeczywiście istnieją\n",
    "    existing_hardware_cols = [col for col in hardware_model_columns if col in filtered_test.columns]\n",
    "    model_features_test = filtered_test[existing_hardware_cols]\n",
    "\n",
    "    # Wyciągnij cechy response (tylko te które istnieją w filtered_test)\n",
    "    # Usuń kolumny kończące się na _duration\n",
    "    available_response_cols = [\n",
    "        'prompt_token_length',\n",
    "        'response_token_length',\n",
    "        'energy_consumption_llm',\n",
    "        'text_standard',\n",
    "        'response'\n",
    "    ]\n",
    "    existing_response_cols = [col for col in available_response_cols if col in filtered_test.columns]\n",
    "    response_features_test = filtered_test[existing_response_cols]\n",
    "\n",
    "    # Połącz cechy\n",
    "    dataframe = pd.concat([prompt_features, model_features_test, response_features_test], axis=1)\n",
    "\n",
    "    dataframe = prepare_data(dataframe)\n",
    "\n",
    "    dataframe.drop(columns=[\"energy_consumption_llm\"], inplace=True)\n",
    "\n",
    "    return dataframe"
   ],
   "id": "85c86f3546f65e8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xd = ready_data(\"codellama:7b\", \"Hello world xd xd. I love python\")\n",
    "\n",
    "xd"
   ],
   "id": "950cb64830e000ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Used columns",
   "id": "2c4bae044e3815d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "used_columns = ['word_count', 'response', 'prompt', 'sentence_count', 'avg_word_length', 'word_diversity',\n",
    "       'unique_word_count', 'avg_sentence_length', 'punctuation_count',\n",
    "       'stop_word_count', 'long_word_count', 'named_entity_count',\n",
    "       'noun_count', 'verb_count', 'adj_count', 'adverb_count',\n",
    "       'pronoun_count', 'prop_adverbs', 'prop_pronouns', 'sentiment_polarity',\n",
    "       'sentiment_subjectivity', 'flesch_reading_ease', 'flesch_kincaid_grade',\n",
    "       'gunning_fog', 'smog_index', 'automated_readability_index',\n",
    "       'coleman_liau_index', 'linsear_write_formula',\n",
    "       'dale_chall_readability_score', 'spache_readability', 'mcalpine_eflaw',\n",
    "       'reading_time', 'fernandez_huerta', 'szigriszt_pazos',\n",
    "       'gutierrez_polini', 'crawford', 'osman', 'gulpease_index',\n",
    "       'wiener_sachtextformel', 'syllable_count', 'lexicon_count',\n",
    "       'char_count', 'letter_count', 'polysyllabcount', 'monosyllabcount',\n",
    "       'question_marks', 'exclamation_marks', 'sentence_embedding_variance',\n",
    "       'personal_pronouns', 'named_entities', 'adjectives', 'adverbs',\n",
    "       'length_x_complexity', 'questions_about_entities',\n",
    "       'desc_complexity_ratio', 'word_count_squared',\n",
    "       'avg_sentence_length_cubed', 'lexical_diversity', 'device_type',\n",
    "       'model_name', 'prompt_token_length', 'response_token_length', 'energy_consumption_llm']"
   ],
   "id": "97d81f62fce74c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train/valid/test",
   "id": "577a86556e052b56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = data[used_columns]\n",
    "\n",
    "data[\"strata\"] = data[\"model_name\"].astype(str) + \"__\" + data[\"device_type\"].astype(str)\n",
    "\n",
    "train, test = train_test_split(\n",
    "    data,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=data[\"strata\"]\n",
    ")\n",
    "\n",
    "# Usuń pomocniczą kolumnę\n",
    "train = train.drop(columns=[\"strata\"])\n",
    "test = test.drop(columns=[\"strata\"])"
   ],
   "id": "e9bcec250a6da338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train.to_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "test.to_csv(os.path.join(\"data\", \"test.csv\"))"
   ],
   "id": "2aef965df21eae2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.shape",
   "id": "7b0b57665f68065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get metrics",
   "id": "4524d7d039fd5288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred) -> dict:\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return dict(zip([\"rmse\", \"mape\", \"mae\", \"r2\"], [rmse, mape, mae, r2]))\n",
    "\n",
    "def smape(y_true, y_pred, epsilon = 1e-8):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) + epsilon\n",
    "\n",
    "    return 100 * np.mean(numerator / denominator)\n",
    "\n",
    "smape_scorer = make_scorer(smape, greater_is_better=False)"
   ],
   "id": "34d65017f439698c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <center>Basic dataset</center>",
   "id": "5e9187929b6369f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = [col for col in train.columns if train[col].dtype == \"object\"]\n",
    "whole_X_train_basic, whole_y_train_basic = train.drop(columns=[\"energy_consumption_llm\"]), train[\"energy_consumption_llm\"]\n",
    "\n",
    "whole_X_train_basic[categories] = whole_X_train_basic[categories].astype(\"category\")\n",
    "\n",
    "X_train_basic, X_test_basic, y_train_basic, y_test_basic = train_test_split(whole_X_train_basic, whole_y_train_basic, test_size=0.3, random_state=42)"
   ],
   "id": "421cdb2db6898c7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "whole_y_train_basic.describe()",
   "id": "5fa18a1626f8c247",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(whole_y_train_basic, bins=50)\n",
    "plt.title('Original Distribution')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(np.log(whole_y_train_basic), bins=50)\n",
    "plt.yscale('log')\n",
    "plt.title('Log Scale Y-axis')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.boxplot(whole_y_train_basic)\n",
    "plt.title('Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check outliers\n",
    "print(f\"99th percentile: {np.percentile(whole_y_train_basic, 99):.6f}\")\n",
    "print(f\"99.9th percentile: {np.percentile(whole_y_train_basic, 99.9):.6f}\")"
   ],
   "id": "a7f248e027393917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "upper_limit = y_train_basic.quantile(0.99)\n",
    "print(f\"Clipping outliers at 99th percentile: {upper_limit:.6f}\")\n",
    "print(f\"Original max: {y_train_basic.max():.6f}\")\n",
    "print(f\"Values being clipped: {(y_train_basic > upper_limit).sum()} out of {len(y_train_basic)}\")"
   ],
   "id": "b842034d03689d50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save model to file and read it",
   "id": "9ee850dc912646cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "def save_model(model: lgb.LGBMRegressor, filename: str):\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    joblib.dump(model, os.path.join(\"models\", filename))\n",
    "\n",
    "def get_model(filename: str) -> lgb.LGBMRegressor:\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"Brak zapisanego modelu o podanej nazwie {filename}\")\n",
    "\n",
    "    return joblib.load(filename)\n",
    "\n",
    "def predict(model_name: str, x_test: np.typing.NDArray[np.float64]) -> np.typing.NDArray[np.float64]:\n",
    "    if not os.path.exists(os.path.join(\"models\", model_name)):\n",
    "        raise FileNotFoundError(f\"Brak zapisanego modelu o podanej nazwie {model_name}\")\n",
    "\n",
    "    model = get_model(model_name)\n",
    "    return model.predict(x_test)"
   ],
   "id": "f32a7e3777539894",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. LightGBM",
   "id": "c1713cdd150df8a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train_clipped = y_train_basic.clip(upper=upper_limit)\n",
    "\n",
    "\n",
    "def define_lgb(trial: optuna.Trial) -> lgb.LGBMRegressor:\n",
    "    params = {\n",
    "        # Regularyzacja\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 5, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 5, log=True),\n",
    "\n",
    "        # Learning\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "\n",
    "        # Tree structure\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 48),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 80),\n",
    "\n",
    "        # Sampling\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"subsample_freq\": 1,\n",
    "\n",
    "        # Quantile regression (mediana - odporna na outliery)\n",
    "        \"objective\": \"quantile\",\n",
    "        \"alpha\": 0.5,  # 0.5 = mediana\n",
    "\n",
    "        \"random_state\": 42,\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "    return lgb.LGBMRegressor(**params)\n",
    "\n",
    "\n",
    "def optimize_lightgbm_basic(trial: optuna.Trial):\n",
    "    lightgbm = define_lgb(trial)\n",
    "    groups = X_train_basic[\"model_name\"]\n",
    "    kf = GroupKFold(\n",
    "        n_splits=len(np.unique(groups)),\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        lightgbm,\n",
    "        X_train_basic,\n",
    "        y_train_clipped,\n",
    "        cv=kf,\n",
    "        groups=groups,\n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    return scores.mean() * (-1)\n",
    "\n",
    "\n",
    "study_lgb_basic = optuna.create_study(\n",
    "    study_name=\"Optimize LGBM\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    ")\n",
    "\n",
    "study_lgb_basic.optimize(optimize_lightgbm_basic, n_trials=100)"
   ],
   "id": "28013cdee6c5a0a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### KROK 3: Po optymalizacji, wytrenuj finalny model",
   "id": "fe83ae93065549f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8a15d10710e1bcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Finalny model z najlepszymi parametrami\n",
    "best_params = study_lgb_basic.best_params\n",
    "best_params.update({\n",
    "    \"random_state\": 42,\n",
    "    \"objective\": \"quantile\",\n",
    "    \"alpha\": 0.5,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\": -1,\n",
    "})\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**best_params)\n",
    "\n",
    "final_model.fit(X_train_basic, y_train_clipped)"
   ],
   "id": "b2f82808f34ae66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"\\nBest parameters:\", study_lgb_basic.best_params)",
   "id": "b5da177b0e0cd3a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### KROK 4: Predykcja i ewaluacja",
   "id": "189b38567e93596f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = final_model.predict(X_test_basic)\n",
    "\n",
    "metrics = get_metrics(y_test_basic, predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL EVALUATION (on original test data)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  {metrics[\"mae\"]:.6f} (relative: {metrics[\"mae\"] / y_test_basic.mean():.2%})\")\n",
    "print(f\"RMSE: {metrics[\"rmse\"]:.6f}\")\n",
    "print(f\"R²:   {metrics[\"r2\"]:.4f}\")\n",
    "print(f\"\\nTest set mean: {y_test_basic.mean():.6f}\")\n",
    "print(f\"Test set std:  {y_test_basic.std():.6f}\")"
   ],
   "id": "64f585017654e310",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Analiza błędów na outlierach",
   "id": "b95543cf70296c94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "errors = np.abs(y_test_basic - predictions)\n",
    "outlier_mask = y_test_basic > upper_limit\n",
    "\n",
    "test_metrics_without_outliers = get_metrics(y_test_basic[~outlier_mask], predictions[~outlier_mask])\n",
    "test_metrics_with_outliers = get_metrics(y_test_basic[outlier_mask], predictions[outlier_mask])\n",
    "print(f\"\\n{'Metric':<20} {'All Test':<15} {'Non-outliers':<15} {'Outliers':<15}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'Count':<20} {len(y_test_basic):<15} {(~outlier_mask).sum():<15} {outlier_mask.sum():<15}\")\n",
    "print(f\"{'MAE':<20} {mae:.6f}       {test_metrics_without_outliers[\"mae\"]:.6f}       {test_metrics_with_outliers[\"mae\"]:.6f}\")\n",
    "print(f\"{'RMSE':<20} {rmse:.6f}       {test_metrics_without_outliers[\"rmse\"]:.6f}       {test_metrics_with_outliers[\"rmse\"]:.6f}\")\n",
    "print(f\"{'R²':<20} {r2:.4f}          {test_metrics_without_outliers[\"r2\"]:.4f}          {test_metrics_with_outliers[\"r2\"]:.4f}\")"
   ],
   "id": "2ab352378c031465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lightGBM_importance = pd.DataFrame({\n",
    "    'feature': final_model.feature_name_,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=lightGBM_importance,\n",
    "    x='importance',\n",
    "    y='feature',\n",
    "    hue='feature',\n",
    "    legend=False,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%g', label_type='edge', fontsize=10, padding=3)\n",
    "\n",
    "plt.title('10 the most important columns for LightGBM', fontsize=16)\n",
    "plt.xlabel('Importance value', fontsize=12)\n",
    "plt.ylabel('Column', fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "89e0049c5c411092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Learning curve",
   "id": "5d13c1b4d21cd3d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "groups = X_train_basic[\"model_name\"]\n",
    "kf = GroupKFold(\n",
    "    n_splits=len(np.unique(groups)),\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    final_model,\n",
    "    X_train_basic,\n",
    "    y_train_clipped,\n",
    "    cv=kf,\n",
    "    groups=groups,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 6),\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores = -train_scores\n",
    "valid_scores = -valid_scores\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), \"o-\", label=\"Train RMSE\")\n",
    "plt.plot(train_sizes, valid_scores.mean(axis=1), \"o-\", label=\"Valid RMSE\")\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                 train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "                 alpha=0.2)\n",
    "plt.fill_between(train_sizes,\n",
    "                 valid_scores.mean(axis=1) - valid_scores.std(axis=1),\n",
    "                 valid_scores.mean(axis=1) + valid_scores.std(axis=1),\n",
    "                 alpha=0.2)\n",
    "\n",
    "plt.title(\"Learning Curve (RMSE)\")\n",
    "plt.xlabel(\"Training samples\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "51c863803c45bad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <center>Optuna visualization</center>",
   "id": "c7c8fdab8b5df785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optuna.visualization.plot_optimization_history(study_lgb_basic)",
   "id": "a3f50e315cfdd1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optuna.visualization.plot_slice(study_lgb_basic)",
   "id": "14c93c5bc2fff242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optuna.visualization.plot_param_importances(study_lgb_basic)",
   "id": "11be4ecdda5f1214",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
